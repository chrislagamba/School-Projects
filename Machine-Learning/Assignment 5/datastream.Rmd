---
title: "Data Stream Analysis - LEARN Assignment 5"
author: "Chris LaGamba"
date: 23 May 2025
output:
  pdf_document: default
bibliography: references.bib
csl: "ama-11.csl"
---

```{r setup, include = FALSE, echo = FALSE}
pacman::p_load(stream, zoo, ggplot2, gridExtra)
```

## Introduction

There has been a significant increase in the amount of patient-generated data in the healthcare sector in the last decade due to the standardization of electronic health records (EHRs) and the incorporation of "wearables" for out-of-hospital monitoring. @henkel2020 Advances in EHRs has allowed for data to be uploaded to the EHR from outside of the care facility, resulting in timely interventions due to the detection of abnormal values. This continuous influx of data is commonly referred to as a "data stream." @sakr2019 In addition, the public health sector has benefited greatly from the use of data streams to monitor hot-spots for outbreaks. This was particularly useful during the start of the COVID-19 pandemic to allow for real-time monitoring and response in communities. Wastewater monitoring was particularly useful at identifying community hot-spots by quantifying the concentrations of the COVID-19 virus in waste water samples. @alhama2022 Other use-cases of data and data streams during the start of the COVID-19 pandemic included aggregating data from public health agencies, such as the European Centre for Disease Prevention and Control, and analysing that data to determine the impact of public health policies to determine if adjustments to existing policies or new policies were needed. @agosto2021 While most of this public health data was available at regular intervals, there existed the possibility of implementing data streams to continuously stream data for analysis and rapid response.

This is typically the case with data available in an EHR. The ability of healthcare teams to react more quickly to a deterioration in patient status based upon this continuous stream of health data is dependent upon establishing appropriate "positive" class(es), classification of the default classifier, and monitoring the prequential error within the data stream. @gama2013 Prequential error is the cumulative average of loss over time. Monitoring of the prequential error can be instantaneous or monitored through "sliding windows" or "fading factors" that only consider the last set time period or number of data points. @gama2013^,^@yan2020. While instantaneous monitoring of the prequential error allows for much more rapid response, it also creates several "false alarms" due to single data points going out of the established monitoring boundaries. Instantaneous monitoring of prequential error is also resource intensive and may not be ideal in all cases due to resource constraints. @gama2013 To mitigate this problem, sliding windows or fading factors may be used to analyse the data and only warn or alarm when data points continue to be out of established boundaries for an established time window. Another issue to consider with data streams is "concept drift," or the change in the input data or target variable over time. @yan2020 This change can be abrupt, such as with sudden drift, or it can be much slower, such as with gradual or incremental drifts. Another consideration is recurrent drift, where old concepts reappear periodically.

To mimic concept drift, a data stream was simulated with defined parameters for data production, classification, monitoring, and warning as well as a pre-defined concept drift occurring at a set position in time. Cumulative and sliding windows were used to trend prequential error at each position in time and to signal warning and drift alarms for concept drift.

## Methods

The parameters of the data stream included: data produced in a stream from N(0 ,2) for the first 1000 data points, a positive class determined by data being \< 2, classification performed by the default classifier (always positive), the stream changes to N(2, 2) after the initial 1000 data points, compute and monitor the prequential error using zero-one loss and four different window models.

The data stream was simulated using the R version 4.5.0 programming language and RStudio version 2025.5.0.496. @manual2025^,^@posit2025 The stream, zoo, ggplot2, and gridExtra packages were loaded into the environment and used to develop, monitor, and plot the data stream and subsequent results. @hahsler2025^,^@zeileis2005^,^@wickham2016 The seed was set to 500 for reproducibility. The data stream was then generated with 1000 initial positions in time ("n_before") followed by 1000 subsequent positions in time ("n_after") along with a total number of positions in time ("total"). The stream was generated by using a combination of formulas for the first 1000 positions in time (stream_before) and the subsequent 1000 positions in time (stream_after) along with "stream" to combine both formulas into a single stream.

```{r environment setup, include = TRUE, message = FALSE, results = FALSE}
# install.packages("pacman")
pacman::p_load(stream, zoo, ggplot2, gridExtra)
library(rmarkdown)
library(knitr)

set.seed(500)

# Generate data stream
n_before <- 1000
n_after <- 1000
total <- n_before + n_after

stream_before <- rnorm(n_before, mean = 0, sd = sqrt(2))
stream_after <- rnorm(n_after, mean = 2, sd = sqrt(2))
stream <- c(stream_before, stream_after)
```

The stream was further defined by creating labels and predictions (true_labels, predictions, loss). The window models (window_models) and error calculations (cummean, cumvar) were also created. The four window models chosen for this project were a cumulative window that included monitoring of all data points in the stream (cumulative) and windows to monitor the previous 100, 200, and 500 positions in time (w100, w200, and w500 respectively).

```{r window models, include = TRUE, message = FALSE, results = FALSE}
# Create labels and predictions
true_labels <- ifelse(stream < 2, 1, 0)
predictions <- rep(1, total)
loss <- ifelse(true_labels == predictions, 0, 1)

# Define window models and error calculations
cummean <- function(x) cumsum(x) / seq_along(x)
cumvar <- function(x) (cumsum(x^2) - cumsum(x)^2 / seq_along(x)) / (seq_along(x) - 1)

window_models <- list(
  cumulative = list(
    error = cummean(loss),
    stdev = sqrt(cumvar(loss))
  ),
  w100 = list(
    error = rollapply(loss, 100, mean, align = "right", fill = NA),
    stdev = rollapply(loss, 100, sd, align = "right", fill = NA)
  ),
  w200 = list(
    error = rollapply(loss, 200, mean, align = "right", fill = NA),
    stdev = rollapply(loss, 200, sd, align = "right", fill = NA)
  ),
  w500 = list(
    error = rollapply(loss, 500, mean, align = "right", fill = NA),
    stdev = rollapply(loss, 500, sd, align = "right", fill = NA)
  )
)
```

A for loop with if-else functions was developed to detect drift at the first identified drift point (first_valid), followed by continued monitoring and a function to evaluate if the current minimum error (current_min_error) and minimum standard deviation (current_min_stdev) are both met. The evaluation function provides if-else functions that determine if the error and standard deviation meet the minimums for both and if so, triggers an if-else function that checks for valid comparisons (e.g., current value greater than the threshold drift) and will provide drift and warning alarms (drift_alarm, warning_alarm) if conditions are met.

```{r detection implementation, include = TRUE, message = FALSE, results = FALSE}
# Detection implementation
alarms <- list()

for (model in names(window_models)) {
  error <- window_models[[model]]$error
  stdev <- window_models[[model]]$stdev
  n <- length(error)
  
  min_error <- rep(NA, n)
  min_stdev <- rep(NA, n)
  warning_alarm <- logical(n)
  drift_alarm <- logical(n)
  
  # Find first valid index with both error and stdev available
  first_valid <- which(!is.na(error) & !is.na(stdev))[1]
  
  if (!is.na(first_valid)) {
    current_min_error <- error[first_valid]
    current_min_stdev <- stdev[first_valid]
    
    for (i in first_valid:n) {
      # Update minimums only with valid values
      if (!is.na(error[i])) {
        current_min_error <- min(current_min_error, error[i], na.rm = TRUE)
      }
      if (!is.na(stdev[i])) {
        current_min_stdev <- min(current_min_stdev, stdev[i], na.rm = TRUE)
      }
      
      min_error[i] <- current_min_error
      min_stdev[i] <- current_min_stdev
      
      # Only evaluate if all components are valid
      if (!is.na(error[i]) && !is.na(stdev[i]) &&
         !is.na(current_min_error) && !is.na(current_min_stdev)) {
        
        threshold_warning <- current_min_error + 2 * current_min_stdev
        threshold_drift <- current_min_error + 3 * current_min_stdev
        current_value <- error[i] + stdev[i]
        
        # Check for valid comparisons
        if (!is.na(current_value)) {
          if (current_value > threshold_drift) {
            drift_alarm[i] <- TRUE
          } else if (current_value > threshold_warning) {
            warning_alarm[i] <- TRUE
          }
        }
      }
    }
  }
  
  alarms[[model]] <- list(
    warning = which(warning_alarm),
    drift = which(drift_alarm)
  )
}
```

## Results

```{r plots, echo = FALSE, message = FALSE, warning = FALSE}
plots <- list()
for (model in names(window_models)) {
  df <- data.frame(
    time = 1:total,
    error = window_models[[model]]$error,
    stdev = window_models[[model]]$stdev
  )
  
  p <- ggplot(df) +
    geom_line(aes(x = time, y = error), color = "blue") +
    geom_vline(xintercept = 1000, color = "red", linetype = "dashed") +
    geom_point(data = df[alarms[[model]]$warning,],
               aes(x = time, y = error), color = "orange", size = 1) +
    geom_point(data = df[alarms[[model]]$drift,],
               aes(x = time, y = error), color = "red", size = 1) +
    labs(title = paste("Window Model:", model),
         x = "Position in Time", y = "Prequential Error") +
    scale_y_continuous(limits = c(0, 0.8)) +
    theme_minimal()
  plots[[model]] <- p
}

grid.arrange(grobs = plots, ncol = 2)
```

The results are presented in graph form with each window represented in an individual graph. The blue line represents the prequential error over time. The orange points represent warning alarms, or statistical warnings that indicate a potential early sign of drift. The red points represent drift alarms when there is a strong drift signal. The red dashed line represents the actual concept drift at position in time 1000 (when drift was coded to occur in this simulation).

The cumulative window tracks the prequential error for all points in time. As it is cumulative, it considers all historical data when triggering alarms (warning or drift). This is evidenced by the much more stable prequential error line over time as the prequential error line remains relatively flat until the actual concept drift at position 1000. The warning and drift alarms are also much slower to activate, with the warning alarm not activing until between positions 1200 and 1250 and the drift alarm activating around position 1900. The prequential error rate is also much slower to increase following the actual concept drift. The cumulative window also has the lowest peak prequential error of all windows at around 0.3.

In contrast, the w100 window (100 points in time) is much more responsive to potential drift as evidenced by the electrocardiogram-like appearance throughout all positions. There are several false alarms between positions 400 and 700 due to the rapid response of the window since it only considers the previous 100 positions when activating an alarm. Additionally, the w100 window signals a warning alarm before concept drift occurs and the drift alarm occurs very soon after concept drift occurs. The w100 window also has the highest peak prequential error of all windows at around 0.7 and is the first window to detect drift, not accounting for the cumulative window that tracks the prequential error at all points in time.

The w200 window (200 points in time) is not as responsive as the w100 window. However, there exists some variability before position 1000 when compared to the cumulative window. Additionally, the w200 window detects drift much more slowly compared to the w100 window. The w200 window also fails to signal a warning alarm before concept drift occurs. The warning alarm occurs around position 1050 and the drift alarm occurs soon after. However, the w200 window is much more responsive compared to the w500 and cumulative windows. There are also no false alarms with this window, indicating more stability when compared to the w100 window.

The w500 window (500 points in time) is much less responsive when compared to the w100 and w200 windows. Its performance at detecting drift is more akin to the stability of the cumulative window, though more responsive in terms of warning and drift alarms. Additionally, prequential error is not detected until around position 500, indicating that it is the slowest of the windows at detecting drift. The w500 window also does not trigger a warning alarm until much longer after the occurrence of concept drift. Apart from the cumulative window, the w500 window is the slowest to signal a drift alarm with the alarm occurring around position 1300. However, this window is much more stable compared to the w100 and w200 windows with less false alarms.

## Discussion

This simulation represents the differences of cumulative and sliding windows at detecting concept drift in a data stream. The cumulative window represents all positions in time and uses all previous points in time to determine prequential error. This contrasts with the other windows that only consider the previous 100, 200, or 500 positions in time. As such, the cumulative window is much slower to respond to concept drift, with warning and drift alarms occurring well after concept drift occurred.

In contrast, the w100 window was much quicker to respond to concept drift, with false alarms occurring well before concept drift due to the sensitivity of only using 100 points for drift detection. However, the w100 window detected concept drift before it occurred with the warning alarm occurring between positions in time 900 and 950 and the drift alarm occurring around position in time 1050. The w200 window is slightly slower to respond, though it is able to trigger warnings for drift soon after the concept drift occurred.

In comparison, the w500 window is much slower to trigger a warning alarm and subsequently a drift alarm. However, this sliding window is much more stable compared to the w100 and w200 windows. The w100 window is very sensitive to drift, with false warning alarms occurring well before concept drift occurred. The w200 window is less responsive than the w100 window; however, there are no false warning alarms before concept drift occurs.

The different sliding windows portray the differences in detecting concept drift based on the size of the sliding window. Although the w100 window is much more prone to false alarms compared to the other windows, it may be an appropriate sliding window to use for detecting concept drift in a model that predicts sepsis using vital signs from a bedside monitor. Early detection of sepsis can make a significant difference in a patient's outcome. However, a sliding window this sensitive may not be as appropriate for a model designed for chronic disease management.

There is also the potential of implementing multiple windows if resources are not limited. For instance, a cumulative window could be used to track all hospital admissions over time for trending purposes. The cumulative window could be combined with a much smaller sliding window that would be more responsive to changes in admissions over the last few days. The smaller sliding window would allow for better predictions at determining hospital bed availability than a cumulative window. However, the cumulative window would offer the ability to "look back" in time to see if there are seasonal trends.

In conclusion, data streams allow for the continuous stream of data into models to make predictions. However, changes may occur over time that can affect model performance. Sliding windows offer the ability to monitor model performance over pre-defined time positions to detect concept drift. Early detection of concept drift is key to improving model performance.

## References
